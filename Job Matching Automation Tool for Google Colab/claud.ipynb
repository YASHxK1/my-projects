{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d19f594",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Job Automation Tool - Fetch and Match Jobs from Multiple Portals\n",
    "Designed for Google Colab Environment\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urlencode, quote\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda153ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class JobAutomationTool:\n",
    "    def __init__(self, profile_json_path=\"profile_data.json\"):\n",
    "        \"\"\"Initialize the job automation tool with user profile data\"\"\"\n",
    "        self.profile_data = self.load_profile_data(profile_json_path)\n",
    "        self.jobs_data = []\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        ]\n",
    "        \n",
    "    def load_profile_data(self, json_path):\n",
    "        \"\"\"Load user profile data from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"‚úÖ Profile loaded successfully!\")\n",
    "            print(f\"Skills: {', '.join(data['Skills'])}\")\n",
    "            print(f\"Desired Roles: {', '.join(data['Desired Roles'])}\")\n",
    "            print(f\"Experience: {data['Experience Level']['Label']} ({data['Experience Level']['Years']} years)\")\n",
    "            print(f\"Location: {data['Desired City']}\")\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading profile data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_random_headers(self):\n",
    "        \"\"\"Get random headers to avoid detection\"\"\"\n",
    "        return {\n",
    "            'User-Agent': random.choice(self.user_agents),\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "    \n",
    "    def calculate_match_score(self, job_title, job_description, company):\n",
    "        \"\"\"Calculate job match percentage based on skills and roles\"\"\"\n",
    "        if not self.profile_data:\n",
    "            return 0\n",
    "            \n",
    "        score = 0\n",
    "        total_criteria = 0\n",
    "        \n",
    "        # Job title matching (40% weight)\n",
    "        title_score = 0\n",
    "        for role in self.profile_data['Desired Roles']:\n",
    "            similarity = fuzz.partial_ratio(role.lower(), job_title.lower())\n",
    "            title_score = max(title_score, similarity)\n",
    "        score += (title_score * 0.4)\n",
    "        total_criteria += 40\n",
    "        \n",
    "        # Skills matching (50% weight)\n",
    "        skills_found = 0\n",
    "        job_text = (job_title + \" \" + job_description).lower()\n",
    "        for skill in self.profile_data['Skills']:\n",
    "            if skill.lower() in job_text:\n",
    "                skills_found += 1\n",
    "        \n",
    "        if self.profile_data['Skills']:\n",
    "            skills_score = (skills_found / len(self.profile_data['Skills'])) * 100\n",
    "            score += (skills_score * 0.5)\n",
    "            total_criteria += 50\n",
    "        \n",
    "        # Company bonus (10% weight)\n",
    "        reputed_companies = ['google', 'microsoft', 'amazon', 'apple', 'meta', 'netflix', 'tesla', 'nvidia']\n",
    "        if any(comp in company.lower() for comp in reputed_companies):\n",
    "            score += 10\n",
    "            total_criteria += 10\n",
    "        \n",
    "        return min(int(score), 100)\n",
    "    \n",
    "    def scrape_naukri_jobs(self, max_jobs=20):\n",
    "        \"\"\"Scrape jobs from Naukri.com\"\"\"\n",
    "        print(\"üîç Scraping Naukri.com...\")\n",
    "        \n",
    "        try:\n",
    "            base_url = \"https://www.naukri.com/jobs-in-{}-{}\".format(\n",
    "                self.profile_data['Desired City'].lower(),\n",
    "                \"-\".join([role.lower().replace(\" \", \"-\") for role in self.profile_data['Desired Roles'][:2]])\n",
    "            )\n",
    "            \n",
    "            headers = self.get_random_headers()\n",
    "            response = requests.get(base_url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                job_cards = soup.find_all(['div', 'article'], class_=re.compile(r'job|result'))[:max_jobs]\n",
    "                \n",
    "                for card in job_cards:\n",
    "                    try:\n",
    "                        title_elem = card.find(['a', 'h2', 'h3'], class_=re.compile(r'title|job'))\n",
    "                        company_elem = card.find(['span', 'div'], class_=re.compile(r'company|org'))\n",
    "                        location_elem = card.find(['span', 'div'], class_=re.compile(r'location|loc'))\n",
    "                        \n",
    "                        if title_elem and company_elem:\n",
    "                            job_title = title_elem.get_text(strip=True)\n",
    "                            company = company_elem.get_text(strip=True)\n",
    "                            location = location_elem.get_text(strip=True) if location_elem else self.profile_data['Desired City']\n",
    "                            \n",
    "                            # Get job link\n",
    "                            job_link = title_elem.get('href', '#')\n",
    "                            if job_link.startswith('/'):\n",
    "                                job_link = 'https://www.naukri.com' + job_link\n",
    "                            \n",
    "                            # Calculate match score\n",
    "                            match_score = self.calculate_match_score(job_title, job_title, company)\n",
    "                            \n",
    "                            if match_score >= 30:  # Only include jobs with >30% match\n",
    "                                self.jobs_data.append({\n",
    "                                    'Job Title': job_title,\n",
    "                                    'Company': company,\n",
    "                                    'Location': location,\n",
    "                                    'Match %': f\"{match_score}%\",\n",
    "                                    'Apply Link': job_link,\n",
    "                                    'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                                    'Source': 'Naukri.com'\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                        \n",
    "                print(f\"‚úÖ Found {len([j for j in self.jobs_data if j['Source'] == 'Naukri.com'])} jobs from Naukri.com\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error scraping Naukri: {e}\")\n",
    "    \n",
    "    def scrape_indeed_jobs(self, max_jobs=20):\n",
    "        \"\"\"Scrape jobs from Indeed\"\"\"\n",
    "        print(\"üîç Scraping Indeed...\")\n",
    "        \n",
    "        try:\n",
    "            # Create search query\n",
    "            query = \"+\".join(self.profile_data['Desired Roles'][:2])\n",
    "            location = self.profile_data['Desired City']\n",
    "            \n",
    "            url = f\"https://in.indeed.com/jobs?q={quote(query)}&l={quote(location)}&fromage=1\"\n",
    "            \n",
    "            headers = self.get_random_headers()\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Indeed job cards\n",
    "                job_cards = soup.find_all('div', class_=re.compile(r'job_seen_beacon|result|slider_container'))[:max_jobs]\n",
    "                \n",
    "                for card in job_cards:\n",
    "                    try:\n",
    "                        title_elem = card.find('a', {'data-jk': True}) or card.find('h2').find('a') if card.find('h2') else None\n",
    "                        company_elem = card.find('span', class_=re.compile(r'companyName'))\n",
    "                        location_elem = card.find('div', class_=re.compile(r'companyLocation'))\n",
    "                        \n",
    "                        if title_elem and company_elem:\n",
    "                            job_title = title_elem.get_text(strip=True)\n",
    "                            company = company_elem.get_text(strip=True)\n",
    "                            location = location_elem.get_text(strip=True) if location_elem else self.profile_data['Desired City']\n",
    "                            \n",
    "                            # Get job link\n",
    "                            job_link = title_elem.get('href', '#')\n",
    "                            if job_link.startswith('/'):\n",
    "                                job_link = 'https://in.indeed.com' + job_link\n",
    "                            \n",
    "                            # Calculate match score\n",
    "                            match_score = self.calculate_match_score(job_title, job_title, company)\n",
    "                            \n",
    "                            if match_score >= 30:\n",
    "                                self.jobs_data.append({\n",
    "                                    'Job Title': job_title,\n",
    "                                    'Company': company,\n",
    "                                    'Location': location,\n",
    "                                    'Match %': f\"{match_score}%\",\n",
    "                                    'Apply Link': job_link,\n",
    "                                    'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                                    'Source': 'Indeed'\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                        \n",
    "                print(f\"‚úÖ Found {len([j for j in self.jobs_data if j['Source'] == 'Indeed'])} jobs from Indeed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error scraping Indeed: {e}\")\n",
    "    \n",
    "    def scrape_linkedin_jobs(self, max_jobs=15):\n",
    "        \"\"\"Scrape jobs from LinkedIn (limited due to restrictions)\"\"\"\n",
    "        print(\"üîç Scraping LinkedIn Jobs...\")\n",
    "        \n",
    "        try:\n",
    "            # LinkedIn is heavily protected, so we'll use a basic approach\n",
    "            keywords = \"+\".join(self.profile_data['Skills'][:3])\n",
    "            location = self.profile_data['Desired City']\n",
    "            \n",
    "            url = f\"https://www.linkedin.com/jobs/search?keywords={quote(keywords)}&location={quote(location)}\"\n",
    "            \n",
    "            headers = self.get_random_headers()\n",
    "            headers['Accept'] = 'text/html,application/xhtml+xml'\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                # Try to find job cards\n",
    "                job_cards = soup.find_all('div', class_=re.compile(r'job|result'))[:max_jobs]\n",
    "                \n",
    "                for card in job_cards:\n",
    "                    try:\n",
    "                        title_elem = card.find('a') or card.find('h3')\n",
    "                        \n",
    "                        if title_elem:\n",
    "                            job_title = title_elem.get_text(strip=True)\n",
    "                            company = \"LinkedIn Job\"  # Placeholder as LinkedIn blocks detailed scraping\n",
    "                            location = self.profile_data['Desired City']\n",
    "                            \n",
    "                            job_link = title_elem.get('href', '#')\n",
    "                            if job_link.startswith('/'):\n",
    "                                job_link = 'https://www.linkedin.com' + job_link\n",
    "                            \n",
    "                            match_score = self.calculate_match_score(job_title, job_title, company)\n",
    "                            \n",
    "                            if match_score >= 25:\n",
    "                                self.jobs_data.append({\n",
    "                                    'Job Title': job_title,\n",
    "                                    'Company': company,\n",
    "                                    'Location': location,\n",
    "                                    'Match %': f\"{match_score}%\",\n",
    "                                    'Apply Link': job_link,\n",
    "                                    'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                                    'Source': 'LinkedIn'\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                        \n",
    "                print(f\"‚úÖ Found {len([j for j in self.jobs_data if j['Source'] == 'LinkedIn'])} jobs from LinkedIn\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå LinkedIn scraping limited due to restrictions: {e}\")\n",
    "    \n",
    "    def add_sample_jobs(self):\n",
    "        \"\"\"Add sample jobs for demonstration\"\"\"\n",
    "        print(\"üîç Adding sample relevant jobs...\")\n",
    "        \n",
    "        sample_jobs = [\n",
    "            {\n",
    "                'Job Title': 'Junior Data Analyst',\n",
    "                'Company': 'TechCorp India',\n",
    "                'Location': 'Pune',\n",
    "                'Match %': '92%',\n",
    "                'Apply Link': 'https://example.com/job1',\n",
    "                'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                'Source': 'Sample Data'\n",
    "            },\n",
    "            {\n",
    "                'Job Title': 'Data Scientist - Entry Level',\n",
    "                'Company': 'Analytics Pro',\n",
    "                'Location': 'Pune',\n",
    "                'Match %': '88%',\n",
    "                'Apply Link': 'https://example.com/job2',\n",
    "                'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                'Source': 'Sample Data'\n",
    "            },\n",
    "            {\n",
    "                'Job Title': 'Risk Analyst - Fresh Graduate',\n",
    "                'Company': 'FinTech Solutions',\n",
    "                'Location': 'Pune',\n",
    "                'Match %': '85%',\n",
    "                'Apply Link': 'https://example.com/job3',\n",
    "                'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                'Source': 'Sample Data'\n",
    "            },\n",
    "            {\n",
    "                'Job Title': 'Business Intelligence Analyst',\n",
    "                'Company': 'Data Insights Ltd',\n",
    "                'Location': 'Pune',\n",
    "                'Match %': '78%',\n",
    "                'Apply Link': 'https://example.com/job4',\n",
    "                'Date Posted': datetime.now().strftime('%d %B %Y'),\n",
    "                'Source': 'Sample Data'\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        self.jobs_data.extend(sample_jobs)\n",
    "        print(f\"‚úÖ Added {len(sample_jobs)} sample jobs\")\n",
    "    \n",
    "    def run_job_search(self):\n",
    "        \"\"\"Run the complete job search process\"\"\"\n",
    "        print(\"üöÄ Starting Job Automation Tool...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        if not self.profile_data:\n",
    "            print(\"‚ùå No profile data loaded. Please check your JSON file.\")\n",
    "            return\n",
    "        \n",
    "        # Clear previous data\n",
    "        self.jobs_data = []\n",
    "        \n",
    "        # Scrape different job portals\n",
    "        try:\n",
    "            self.scrape_naukri_jobs()\n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "            self.scrape_indeed_jobs()\n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "            self.scrape_linkedin_jobs()\n",
    "            time.sleep(2)  # Rate limiting\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Some scraping errors occurred: {e}\")\n",
    "        \n",
    "        # Add sample jobs for demonstration\n",
    "        self.add_sample_jobs()\n",
    "        \n",
    "        # Remove duplicates and sort by match score\n",
    "        self.remove_duplicates()\n",
    "        self.sort_jobs_by_match()\n",
    "        \n",
    "        # Save to CSV\n",
    "        self.save_to_csv()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"‚úÖ Job search completed successfully!\")\n",
    "        print(f\"üìä Total jobs found: {len(self.jobs_data)}\")\n",
    "        \n",
    "        if self.jobs_data:\n",
    "            avg_match = sum(int(job['Match %'].replace('%', '')) for job in self.jobs_data) / len(self.jobs_data)\n",
    "            print(f\"üìà Average match score: {avg_match:.1f}%\")\n",
    "    \n",
    "    def remove_duplicates(self):\n",
    "        \"\"\"Remove duplicate jobs based on title and company\"\"\"\n",
    "        seen = set()\n",
    "        unique_jobs = []\n",
    "        \n",
    "        for job in self.jobs_data:\n",
    "            identifier = (job['Job Title'].lower(), job['Company'].lower())\n",
    "            if identifier not in seen:\n",
    "                seen.add(identifier)\n",
    "                unique_jobs.append(job)\n",
    "        \n",
    "        self.jobs_data = unique_jobs\n",
    "        print(f\"üîÑ Removed duplicates. Unique jobs: {len(self.jobs_data)}\")\n",
    "    \n",
    "    def sort_jobs_by_match(self):\n",
    "        \"\"\"Sort jobs by match percentage (highest first)\"\"\"\n",
    "        self.jobs_data.sort(key=lambda x: int(x['Match %'].replace('%', '')), reverse=True)\n",
    "    \n",
    "    def save_to_csv(self, filename=None):\n",
    "        \"\"\"Save jobs data to CSV file\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"matched_jobs_{timestamp}.csv\"\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(self.jobs_data)\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"üíæ Jobs saved to: {filename}\")\n",
    "            \n",
    "            # Display top 5 jobs\n",
    "            if len(df) > 0:\n",
    "                print(\"\\nüèÜ Top 5 Matched Jobs:\")\n",
    "                print(\"-\" * 80)\n",
    "                for i, row in df.head().iterrows():\n",
    "                    print(f\"{i+1}. {row['Job Title']} at {row['Company']} - {row['Match %']} match\")\n",
    "                    print(f\"   üìç {row['Location']} | üåê {row['Source']}\")\n",
    "                    print(f\"   üîó {row['Apply Link']}\")\n",
    "                    print(\"-\" * 80)\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving to CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def display_stats(self):\n",
    "        \"\"\"Display job search statistics\"\"\"\n",
    "        if not self.jobs_data:\n",
    "            print(\"No jobs data available.\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.jobs_data)\n",
    "        \n",
    "        print(\"\\nüìä Job Search Statistics:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"Total Jobs Found: {len(df)}\")\n",
    "        print(f\"Average Match Score: {df['Match %'].str.replace('%', '').astype(int).mean():.1f}%\")\n",
    "        print(f\"Jobs by Source:\")\n",
    "        print(df['Source'].value_counts().to_string())\n",
    "        print(f\"\\nTop Companies:\")\n",
    "        print(df['Company'].value_counts().head().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e0f25",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the job automation tool\"\"\"\n",
    "    \n",
    "    # Install required packages (for Google Colab)\n",
    "    print(\"üîß Installing required packages...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    \n",
    "    packages = ['beautifulsoup4', 'requests', 'pandas', 'fuzzywuzzy', 'python-levenshtein']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "        except:\n",
    "            print(f\"‚ö†Ô∏è  Could not install {package}\")\n",
    "    \n",
    "    print(\"‚úÖ Packages installed!\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ JOB AUTOMATION TOOL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize and run the tool\n",
    "    try:\n",
    "        tool = JobAutomationTool(\"profile_data.json\")\n",
    "        tool.run_job_search()\n",
    "        tool.display_stats()\n",
    "        \n",
    "        print(\"\\nüéâ Job automation completed successfully!\")\n",
    "        print(\"üìÅ Check your files for the generated CSV with job listings.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error running job automation: {e}\")\n",
    "        print(\"Please ensure your profile_data.json file is properly formatted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SECTION - Edit these values as needed\n",
    "# =============================================================================\n",
    "\n",
    "def update_profile_config():\n",
    "    \"\"\"Update profile configuration if needed\"\"\"\n",
    "    config = {\n",
    "        \"Skills\": [\n",
    "            \"Data Analysis\",\n",
    "            \"Python\", \n",
    "            \"SQL\",\n",
    "            \"Machine Learning\",\n",
    "            \"Power BI\",\n",
    "            \"Excel\",\n",
    "            \"Statistics\",\n",
    "            \"Tableau\"\n",
    "        ],\n",
    "        \"Desired Roles\": [\n",
    "            \"Data Analyst\",\n",
    "            \"Junior Data Scientist\", \n",
    "            \"Risk Analyst\",\n",
    "            \"Business Analyst\",\n",
    "            \"Research Analyst\"\n",
    "        ],\n",
    "        \"Experience Level\": {\n",
    "            \"Label\": \"Fresher\",\n",
    "            \"Years\": 0\n",
    "        },\n",
    "        \"Desired City\": \"Pune\"\n",
    "    }\n",
    "    \n",
    "    with open('profile_data.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "    \n",
    "    print(\"‚úÖ Profile configuration updated!\")\n",
    "\n",
    "# Uncomment the line below to update your profile configuration\n",
    "# update_profile_config()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
